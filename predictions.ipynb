{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet 3 - Anticipez les besoins en consommation de bâtiments\n",
    "\n",
    "## Notebook de predictions\n",
    "\n",
    "Le but de ce notebook est d'utiliser le dataset clean généré par l'analyse exploratoire, et de créer des modèles prédictifs pour les consommations énergétiques et l'émission de CO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MLUtils import DataAnalysis, DataEngineering\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation du jeu de données\n",
    "df = pd.read_csv('data/clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisation des données avec MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sélection des colonnes numériques\n",
    "numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Sélection des colonnes non numériques\n",
    "non_numeric_columns = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "# Création du scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Application de la normalisation sur les colonnes numériques\n",
    "scaled_numeric_data = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Création d'un DataFrame pour les données normalisées\n",
    "df_scaled_numeric = pd.DataFrame(scaled_numeric_data, columns=numeric_columns)\n",
    "\n",
    "# Combinaison des données numériques normalisées avec les données non numériques\n",
    "df_scaled = pd.concat([df_scaled_numeric, df[non_numeric_columns].reset_index()], axis=1)\n",
    "\n",
    "# Affichage des premières lignes pour vérifier la création de df_scaled\n",
    "print(df_scaled.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Dropper les lignes sans energystarscore.\n",
    "Faire une prédiction de la colonne TotalGHGEmission.\n",
    "Ensuite faire le feature importance (mesurer les features qui ont le plus d'impact).\n",
    "Puis, déterminer si la colonne energystarscore est importante ou pas.\n",
    "Conclure sur l'utilité de garder cette colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remplissage de la colonne ENERGYSTARScore en fonction des autres colonnes.\n",
    "\n",
    "Le dataset ne contenait pas tous les scores ENERGYSTAR. Nous allons tout d'abord créer et appliquer un modèle qui va remplir cette colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Séparation du DataFrame en deux ensembles\n",
    "df_scaled_with_score = df_scaled[df_scaled['ENERGYSTARScore'].notna()]\n",
    "df_scaled_without_score = df_scaled[df_scaled['ENERGYSTARScore'].isna()]\n",
    "\n",
    "# Séparation des caractéristiques (X) et de la cible (y) - Assurez-vous de retirer 'ENERGYSTARScore'\n",
    "X = df_scaled_with_score.drop('ENERGYSTARScore', axis=1)\n",
    "y = df_scaled_with_score['ENERGYSTARScore']\n",
    "\n",
    "# Division en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Création et entraînement du modèle\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation du modèle\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "\n",
    "# alpha et l1_ratio doivent être ajustés en fonction de vos données\n",
    "elastic_net_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "elastic_net_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred = elastic_net_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Création et entraînement du modèle Lasso\n",
    "lasso_model = Lasso(alpha=0.1, random_state=42)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred_lasso = lasso_model.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "print(f\"Mean Squared Error (Lasso): {mse_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Création et entraînement du modèle Ridge\n",
    "ridge_model = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred_ridge = ridge_model.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(f\"Mean Squared Error (Ridge): {mse_ridge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Création et entraînement du modèle SVR\n",
    "svr_model = SVR(kernel='rbf')\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred_svr = svr_model.predict(X_test)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "print(f\"Mean Squared Error (SVR): {mse_svr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Création et entraînement du modèle XGBoost\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions et évaluation\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"Mean Squared Error (XGBoost): {mse_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle XGBoost donne les meilleurs résultats. TODO: On l'utilise pour remplir les valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the resulting dataframe to a csv file\n",
    "df.to_csv('data/clean_with_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On reprend le dataset original, pour déterminer si la colonne energystarscore est pertinente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On reprend le csv initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty lines for the column 'ENERGYSTARScore'\n",
    "df = df[df['ENERGYSTARScore'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataAnalysis.show_columns_population(df, type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Après avoir enlever les observations ayant un EnergyStarScore vide, nous obtenons un jeu de 2524 données.\n",
    "Nous allons maintenant faire une prédiction pour la colonne TotalGHGEmissions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "y = df['TotalGHGEmissions']\n",
    "X = df.drop('TotalGHGEmissions', axis=1)\n",
    "X = X.drop('SiteEnergyUseWN(kBtu)', axis=1)\n",
    "X = X.drop('SiteEnergyUse(kBtu)', axis=1)\n",
    "X = X.drop('GHGEmissionsIntensity', axis=1)\n",
    "X = X.drop('SourceEUI(kBtu/sf)', axis=1)\n",
    "X = X.drop('SourceEUIWN(kBtu/sf)', axis=1)\n",
    "X = X.drop('SiteEUI(kBtu/sf)', axis=1)\n",
    "X = X.drop('SiteEUIWN(kBtu/sf)', axis=1)\n",
    "# X = X.drop('ENERGYSTARScore', axis=1)\n",
    "X = X.drop('EnergyUse_Age_Ratio', axis=1)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Computing feature importance\n",
    "feature_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance.importances_mean})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Drop negative values from the DataFrame\n",
    "importance_df = importance_df[importance_df['importance'] > 0]\n",
    "\n",
    "# Sorting values for better visualization\n",
    "importance_df_sorted = importance_df.sort_values(by='importance')\n",
    "\n",
    "# Creating the plot\n",
    "fig = px.bar(importance_df_sorted, x='importance', y='feature', orientation='h', title=\"Feature Importances in Predicting Total GHG Emissions\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons prendre en considération les features importantes uniquement pour la suite de nos recherches de modèles de prédiction.\n",
    "Etant donné l'aspect subjectif de la colonne ENERGYSTARScore (et malgré sa relative importance pour prédire les émissions de co²), nous allons aussi l'enlever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "y = df['SiteEnergyUse(kBtu)']\n",
    "X = df.drop('TotalGHGEmissions', axis=1)\n",
    "X = X.drop('SiteEnergyUseWN(kBtu)', axis=1)\n",
    "X = X.drop('SiteEnergyUse(kBtu)', axis=1)\n",
    "X = X.drop('GHGEmissionsIntensity', axis=1)\n",
    "X = X.drop('SourceEUI(kBtu/sf)', axis=1)\n",
    "X = X.drop('SourceEUIWN(kBtu/sf)', axis=1)\n",
    "X = X.drop('SiteEUI(kBtu/sf)', axis=1)\n",
    "X = X.drop('SiteEUIWN(kBtu/sf)', axis=1)\n",
    "# X = X.drop('ENERGYSTARScore', axis=1)\n",
    "X = X.drop('EnergyUse_Age_Ratio', axis=1)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating and training the Random Forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting and evaluating the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Computing feature importance\n",
    "feature_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "importance_df = pd.DataFrame({'feature': X.columns, 'importance': feature_importance.importances_mean})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Drop negative values from the DataFrame\n",
    "importance_df = importance_df[importance_df['importance'] > 0]\n",
    "\n",
    "# Sorting values for better visualization\n",
    "importance_df_sorted = importance_df.sort_values(by='importance')\n",
    "\n",
    "# Creating the plot\n",
    "fig = px.bar(importance_df_sorted, x='importance', y='feature', orientation='h', title=\"Feature Importances in Predicting Site Energy Use (kBtu)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos analyses nous permettent de constater que les colonnes utiles pour les prédictions de TotalGHGEmissions (Emission de co²) et de SiteEnergyUse(kBtu) (Utilisation totale d'énergie) sont les suivantes :\n",
    "- NumberofFloors\n",
    "- NaturalGas(therms)\n",
    "- PropertyGFAParking\n",
    "- NaturalGas(kBtu)\n",
    "- PrimaryPropertyType\n",
    "- BuildingType\n",
    "- Neighborhood_DOWNTOWN\n",
    "- Neighborhood_GREATER DUWAMISH\n",
    "- NumberofBuildings\n",
    "- SteamUse(kBtu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean.csv')\n",
    "\n",
    "# Only keep the following columns : NumberofFloors, NaturalGas(therms), PropertyGFAParking, NaturalGas(kBtu), PrimaryPropertyType, BuildingType, Neighborhood_DOWNTOWN, Neighborhood_GREATER DUWAMISH, NumberofBuildings, SteamUse(kBtu)\n",
    "df = df[['TotalGHGEmissions', 'SiteEnergyUse(kBtu)', 'NumberofFloors', 'NaturalGas(therms)', 'PropertyGFAParking', 'NaturalGas(kBtu)', 'PrimaryPropertyType', 'BuildingType', 'Neighborhood_DOWNTOWN', 'Neighborhood_GREATER DUWAMISH', 'NumberofBuildings', 'SteamUse(kBtu)']]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a GridSearch to find the best parameters for the model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "y = df['TotalGHGEmissions']\n",
    "X = df.drop('TotalGHGEmissions', axis=1)\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model_rfr = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid_rfr = {\n",
    "    'n_estimators': [20,30,40],\n",
    "    'max_depth': [12,13,11],\n",
    "    'max_features': [5, 6, 7]\n",
    "}\n",
    "\n",
    "grid_search_rfr = GridSearchCV(estimator=model_rfr, param_grid=param_grid_rfr, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search_rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the grid_search_gbr.cv_results_ to a DataFrame\n",
    "results = pd.DataFrame(grid_search_rfr.cv_results_)\n",
    "\n",
    "# Select and rename columns for easier reading\n",
    "results = results[['param_n_estimators', 'param_max_depth', 'param_max_features', 'mean_test_score']]\n",
    "results.columns = ['N Estimators', 'Max Depth', 'Max Features', 'Mean Test Score']\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "results_melted = results.melt(id_vars='Mean Test Score', var_name='Parameter', value_name='Value')\n",
    "\n",
    "# Create the FacetGrid\n",
    "g = sns.FacetGrid(results_melted, col='Parameter', sharex=False, sharey=False, col_wrap=2)\n",
    "g = g.map(sns.scatterplot, 'Value', 'Mean Test Score')\n",
    "\n",
    "# Add titles and adjust layout\n",
    "g.set_titles(col_template=\"{col_name}\", fontweight='bold', fontsize=14)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('GridSearchCV Results Across Different Parameters', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming that grid_search.cv_results_ is defined elsewhere and contains the grid search results\n",
    "cv_results = pd.DataFrame(grid_search_rfr.cv_results_)\n",
    "\n",
    "cv_results['param_n_estimators'] = cv_results['param_n_estimators'].astype(int)\n",
    "cv_results['param_max_features'] = cv_results['param_max_features'].astype(str)\n",
    "\n",
    "pivot_df = cv_results.pivot_table(index='param_n_estimators', columns='param_max_features', values='mean_test_score')\n",
    "\n",
    "pivot_df.index = pivot_df.index.astype(str)\n",
    "pivot_df.columns = pivot_df.columns.astype(str)\n",
    "\n",
    "# Create a new DataFrame to hold the percentage format text\n",
    "text = [[f\"{val:.2%}\" for val in row] for row in pivot_df.values]\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=pivot_df.values,\n",
    "    x=pivot_df.columns.tolist(),\n",
    "    y=pivot_df.index.tolist(),\n",
    "    colorscale='RdYlGn',\n",
    "    reversescale=False,\n",
    "    text=text,  # Add the percentage text\n",
    "    texttemplate=\"%{text}\",  # Use the text from the text argument\n",
    "    hoverinfo=\"z+text\"  # Show the percentage text on hover\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Grid Search Results',\n",
    "    xaxis_title='max_features',\n",
    "    yaxis_title='n_estimators',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# New parameter grid for GradientBoostingRegressor\n",
    "param_grid_gbr = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search_gbr = GridSearchCV(estimator=model_gbr, param_grid=param_grid_gbr, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Training the model\n",
    "grid_search_gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the grid_search_gbr.cv_results_ to a DataFrame\n",
    "results = pd.DataFrame(grid_search_gbr.cv_results_)\n",
    "\n",
    "# Filter the columns for plotting\n",
    "plot_data = results.filter(regex='(param_n_estimators|param_learning_rate|mean_test_score)')\n",
    "\n",
    "# Rename columns for easier reading\n",
    "plot_data.rename(columns={\n",
    "    'param_n_estimators': 'N Estimators',\n",
    "    'param_learning_rate': 'Learning Rate',\n",
    "    'mean_test_score': 'Mean Test Score'\n",
    "}, inplace=True)\n",
    "\n",
    "# Create a 3D scatter plot\n",
    "fig = px.scatter_3d(\n",
    "    plot_data, \n",
    "    x='N Estimators', \n",
    "    y='Learning Rate', \n",
    "    z='Mean Test Score', \n",
    "    color='Mean Test Score', \n",
    "    title='GridSearchCV Results'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Creating a pivot table for the heatmap\n",
    "pivot_table = results.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_n_estimators',\n",
    "    columns='param_learning_rate'\n",
    ")\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlGnBu')\n",
    "plt.title('GridSearchCV Mean Test Scores')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('N Estimators')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the grid_search_gbr.cv_results_ to a DataFrame\n",
    "results = pd.DataFrame(grid_search_gbr.cv_results_)\n",
    "\n",
    "# Select and rename columns for easier reading\n",
    "results = results[['param_n_estimators', 'param_learning_rate', 'param_max_depth', 'param_max_features', 'mean_test_score']]\n",
    "results.columns = ['N Estimators', 'Learning Rate', 'Max Depth', 'Max Features', 'Mean Test Score']\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "results_melted = results.melt(id_vars='Mean Test Score', var_name='Parameter', value_name='Value')\n",
    "\n",
    "# Create the FacetGrid\n",
    "g = sns.FacetGrid(results_melted, col='Parameter', sharex=False, sharey=False, col_wrap=2)\n",
    "g = g.map(sns.scatterplot, 'Value', 'Mean Test Score')\n",
    "\n",
    "# Add titles and adjust layout\n",
    "g.set_titles(col_template=\"{col_name}\", fontweight='bold', fontsize=14)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('GridSearchCV Results Across Different Parameters', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affine les paramètres selon cette dernière visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model_gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# New parameter grid for GradientBoostingRegressor\n",
    "param_grid_gbr = {\n",
    "    'n_estimators': [200, 300, 400],\n",
    "    'learning_rate': [0.1, 0.15, 0.2],\n",
    "    'max_depth': [5, 6 ,7],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search_gbr = GridSearchCV(estimator=model_gbr, param_grid=param_grid_gbr, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Training the model\n",
    "grid_search_gbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the grid_search_gbr.cv_results_ to a DataFrame\n",
    "results = pd.DataFrame(grid_search_gbr.cv_results_)\n",
    "\n",
    "# Select and rename columns for easier reading\n",
    "results = results[['param_n_estimators', 'param_learning_rate', 'param_max_depth', 'param_max_features', 'mean_test_score']]\n",
    "results.columns = ['N Estimators', 'Learning Rate', 'Max Depth', 'Max Features', 'Mean Test Score']\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "results_melted = results.melt(id_vars='Mean Test Score', var_name='Parameter', value_name='Value')\n",
    "\n",
    "# Create the FacetGrid\n",
    "g = sns.FacetGrid(results_melted, col='Parameter', sharex=False, sharey=False, col_wrap=2)\n",
    "g = g.map(sns.scatterplot, 'Value', 'Mean Test Score')\n",
    "\n",
    "# Add titles and adjust layout\n",
    "g.set_titles(col_template=\"{col_name}\", fontweight='bold', fontsize=14)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('GridSearchCV Results Across Different Parameters', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oc-p3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
